{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRw6LMezIXIU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 2: Training your own ML Model\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/peckjon/hosting-ml-as-microservice/blob/master/part2/train_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7VOd1wWIXIX"
   },
   "source": [
    "### Download corpuses\n",
    "\n",
    "We'll continue using the `movie_reviews` corpus to train our model. The `stopwords` corpus contains a [set of standard stopwords](https://gist.github.com/sebleier/554280) we'll want to remove from the input, and `punkt` is used for toneization in the [.words()](https://www.nltk.org/api/nltk.corpus.html#corpus-reader-functions) method of the corpus reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0Y1I9OhYIXIX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\kandp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kandp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kandp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download\n",
    "\n",
    "download('movie_reviews')\n",
    "download('punkt')\n",
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywQhZnDjIXIY"
   },
   "source": [
    "### Define feature extractor and bag-of-words converter\n",
    "\n",
    "Given a list of (already tokenized) words, we need a function to extract just the ones we care about: those not found in the list of English stopwords or standard punctuation.\n",
    "\n",
    "We also need a way to easily turn a list of words into a [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model), pairing each word with the count of its occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EZnRBIu9IXIZ"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "stopwords_eng = stopwords.words('english')\n",
    "\n",
    "def extract_features(words):\n",
    "    return [w for w in words if w not in stopwords_eng and w not in punctuation]\n",
    "\n",
    "def bag_of_words(words):\n",
    "    bag = {}\n",
    "    for w in words:\n",
    "        bag[w] = bag.get(w,0)+1\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0KfZ29qIXIZ"
   },
   "source": [
    "### Ingest, clean, and convert the positive and negative reviews\n",
    "\n",
    "For both the positive (\"pos\") and negative (\"neg\") sets of reviews, extract the features and convert to bag of words. From these, we construct a list of tuples known as a \"featureset\": the first part of each tuple is the bag of words for that review, and the second is its label (\"pos\"/\"neg\").\n",
    "\n",
    "Note that `movie_reviews.words(fileid)` provides a tokenized list of words. If we wanted the un-tokenized text, we would use `movie_reviews.raw(fileid)` instead, then tokenize it using our preferred tokenizeer (e.g. [nltk.tokenize.word_tokenize](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.punkt.PunktLanguageVars.word_tokenize))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xCJh2YMSIXIa"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "reviews_pos = []\n",
    "reviews_neg = []\n",
    "for fileid in movie_reviews.fileids('pos'):\n",
    "    words = extract_features(movie_reviews.words(fileid))\n",
    "    reviews_pos.append((bag_of_words(words), 'pos'))\n",
    "for fileid in movie_reviews.fileids('neg'):\n",
    "    words = extract_features(movie_reviews.words(fileid))\n",
    "    reviews_neg.append((bag_of_words(words), 'neg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiIJdhNxIXIa"
   },
   "source": [
    "### Split reviews into training and test sets\n",
    "We need to break up each group of reviews into a training set (about 80%) and a test set (the remaining 20%). In case there's some meaningful order to the reviews (e.g. the first 800 are from one group of reviewers, the next 200 are from another), we shuffle the sets first to ensure we aren't introducing additional bias. Note that this means our accuracy will not be exactly the same on every run; if you wish to see consistent results on each run, you can stabilize the shuffle by calling [random.seed(n)](https://www.geeksforgeeks.org/random-seed-in-python/) first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def split_idx(size, split_pct):\n",
    "#     train_size = int(size*split_pct)\n",
    "#     total_idx = list(range(size))\n",
    "#     train_idx = random.sample(total_idx, train_size)\n",
    "#     test_idx = list(set(total_idx).difference(set(train_idx)))\n",
    "#     return (train_idx, test_idx)\n",
    "\n",
    "# train_idx, test_idx = split_idx(len(reviews_pos), 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_train, pos_test = reviews_pos[train_idx], reviews_pos[test_idx]\n",
    "# neg_train, neg_tes = reviews_neg[train_idx], reviews_neg[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1O6DuyEkIXIb"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "split_pct = .80\n",
    "\n",
    "def split_set(review_set):\n",
    "    split = int(len(review_set)*split_pct)\n",
    "    return (review_set[:split], review_set[split:])\n",
    "\n",
    "shuffle(reviews_pos)\n",
    "shuffle(reviews_neg)\n",
    "\n",
    "pos_train, pos_test = split_set(reviews_pos)\n",
    "neg_train, neg_test = split_set(reviews_neg)\n",
    "\n",
    "train_set = pos_train+neg_train\n",
    "test_set = pos_test+neg_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLGp3GLAIXIb"
   },
   "source": [
    "### Train the model\n",
    "\n",
    "Now that our data is ready, the training step itself is quite simple if we use the [NaiveBayesClassifier](https://www.nltk.org/api/nltk.classify.html#module-nltk.classify.naivebayes) provided by NLTK.\n",
    "\n",
    "If you are used to methods such as `model.fit(x,y)` which take two parameters -- the data and the labels -- it may be confusing that `NaiveBayesClassifier.train` takes just one argument. This is because the labels are already embedded in `train_set`: each element in the set is a Bag of Words paired with a 'pos' or 'neg'; value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wa2Ea14pIXIc"
   },
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "model = NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZbbihm0IXIc"
   },
   "source": [
    "### Check model accuracy\n",
    "\n",
    "NLTK's built-in [accuracy](https://www.nltk.org/api/nltk.classify.html#module-nltk.classify.util) utility can run our test_set through the model and compare the labels returned by the model to the labels in the test set, producing an overall % accuracy. Not too impressive, right? We need to improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "oa-3ZHR8IXIc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.75\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.util import accuracy\n",
    "\n",
    "print(100 * accuracy(model, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SXf-ci1IXIc"
   },
   "source": [
    "### Save the model\n",
    "Our trained model will be cleared from memory when this notebook is closed. So that we can use it again later, save the model as a file using the [pickle](https://docs.python.org/3/library/pickle.html) serializer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "d4ShsBjmIXId",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model_file = open('sa_classifier.pickle','wb')\n",
    "pickle.dump(model, model_file)\n",
    "model_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Baseline\n",
    "\n",
    "let us try improving the baseline accuracy further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text processing utils\n",
    "\n",
    "The following functions encapsulate some operations that we will repeat on multiple implementations. Extracting them to independent functions allows us to reduce unnecessary duplication and increase code readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_useful_word(word):\n",
    "    return (word not in stopwords_eng) and (word not in punctuation)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Everygram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'hello_world', 'hello_world_corpuses', 'world', 'world_corpuses', 'world_corpuses_calling', 'corpuses', 'corpuses_calling', 'calling']\n"
     ]
    }
   ],
   "source": [
    "# IMPLEMENTATION: use n-grams\n",
    "\n",
    "import re\n",
    "# from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "\n",
    "def extract_features_evgram(document):\n",
    "    document = document.lower()\n",
    "    document = re.sub(r'[^a-zA-Z0-9\\s]', ' ', document)\n",
    "    words = [w for w in document.split(\" \") if w != \"\" and is_useful_word(w)]\n",
    "    return ['_'.join(ngram) for ngram in list(everygrams(words, max_len=3))]\n",
    "\n",
    "# Example\n",
    "print(extract_features_evgram(\"Hello world, corpuses calling!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0KfZ29qIXIZ"
   },
   "source": [
    "### Ingest, clean, and convert the positive and negative reviews\n",
    "\n",
    "use everygram on the same process as before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "xCJh2YMSIXIa"
   },
   "outputs": [],
   "source": [
    "reviews_pos_evg = []\n",
    "reviews_neg_evg = []\n",
    "for fileid in movie_reviews.fileids('pos'):\n",
    "    words = extract_features_evgram(movie_reviews.raw(fileid))\n",
    "    reviews_pos_evg.append((bag_of_words(words), 'pos'))\n",
    "for fileid in movie_reviews.fileids('neg'):\n",
    "    words = extract_features_evgram(movie_reviews.raw(fileid))\n",
    "    reviews_neg_evg.append((bag_of_words(words), 'neg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiIJdhNxIXIa"
   },
   "source": [
    "### Split reviews into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "1O6DuyEkIXIb"
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "shuffle(reviews_pos_evg)\n",
    "shuffle(reviews_neg_evg)\n",
    "\n",
    "pos_train, pos_test = split_set(reviews_pos_evg)\n",
    "neg_train, neg_test = split_set(reviews_neg_evg)\n",
    "\n",
    "train_set = pos_train+neg_train\n",
    "test_set = pos_test+neg_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLGp3GLAIXIb"
   },
   "source": [
    "### Train the model\n",
    "\n",
    "Retraining the NaiveBayesClassifier model on the new data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "wa2Ea14pIXIc"
   },
   "outputs": [],
   "source": [
    "model_nb_evg = NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZbbihm0IXIc"
   },
   "source": [
    "### Check model accuracy\n",
    "\n",
    "NLTK's built-in [accuracy](https://www.nltk.org/api/nltk.classify.html#module-nltk.classify.util) utility can run our test_set through the model and compare the labels returned by the model to the labels in the test set, producing an overall % accuracy. Not too impressive, right? We need to improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "oa-3ZHR8IXIc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.5\n"
     ]
    }
   ],
   "source": [
    "print(100 * accuracy(model_nb_evg, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SXf-ci1IXIc"
   },
   "source": [
    "### Save the model\n",
    "Our trained model will be cleared from memory when this notebook is closed. So that we can use it again later, save the model as a file using the [pickle](https://docs.python.org/3/library/pickle.html) serializer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "d4ShsBjmIXId",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_file = open('sa_classifier_evg.pickle','wb')\n",
    "pickle.dump(model, model_file)\n",
    "model_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\kandp\\anaconda3\\lib\\site-packages (3.4.4)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy) (61.2.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy) (8.1.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy) (1.10.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.1.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Collecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.4.1) (3.4.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.27.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (61.2.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\kandp\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'corpus', 'call']\n"
     ]
    }
   ],
   "source": [
    "# IMPLEMENTATION: use Spacy lemmatizer\n",
    "\n",
    "import spacy \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "def extract_features_lemm(document):\n",
    "    return [str(w.lemma_) for w in nlp(document) if is_useful_word(w.text)]\n",
    "\n",
    "# Example:\n",
    "print(extract_features_lemm(\"Hello world, corpuses calling!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0KfZ29qIXIZ"
   },
   "source": [
    "### Ingest, clean, and convert the positive and negative reviews\n",
    "\n",
    "use lemmatized data on the same process as before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "xCJh2YMSIXIa"
   },
   "outputs": [],
   "source": [
    "reviews_pos_lemm = []\n",
    "reviews_neg_lemm = []\n",
    "for fileid in movie_reviews.fileids('pos'):\n",
    "    words = extract_features_lemm(movie_reviews.raw(fileid))\n",
    "    reviews_pos_lemm.append((bag_of_words(words), 'pos'))\n",
    "for fileid in movie_reviews.fileids('neg'):\n",
    "    words = extract_features_lemm(movie_reviews.raw(fileid))\n",
    "    reviews_neg_lemm.append((bag_of_words(words), 'neg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiIJdhNxIXIa"
   },
   "source": [
    "### Split reviews into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "1O6DuyEkIXIb"
   },
   "outputs": [],
   "source": [
    "shuffle(reviews_pos_lemm)\n",
    "shuffle(reviews_neg_lemm)\n",
    "\n",
    "pos_train, pos_test = split_set(reviews_pos_lemm)\n",
    "neg_train, neg_test = split_set(reviews_neg_lemm)\n",
    "\n",
    "train_set = pos_train+neg_train\n",
    "test_set = pos_test+neg_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLGp3GLAIXIb"
   },
   "source": [
    "### Train the model\n",
    "\n",
    "Retraining the NaiveBayesClassifier model on the new data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "wa2Ea14pIXIc"
   },
   "outputs": [],
   "source": [
    "model_nb_lemm = NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZbbihm0IXIc"
   },
   "source": [
    "### Check model accuracy\n",
    "\n",
    "NLTK's built-in [accuracy](https://www.nltk.org/api/nltk.classify.html#module-nltk.classify.util) utility can run our test_set through the model and compare the labels returned by the model to the labels in the test set, producing an overall % accuracy. Not too impressive, right? We need to improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "oa-3ZHR8IXIc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.util import accuracy\n",
    "\n",
    "print(100 * accuracy(model_nb_lemm, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SXf-ci1IXIc"
   },
   "source": [
    "### Save the model\n",
    "Our trained model will be cleared from memory when this notebook is closed. So that we can use it again later, save the model as a file using the [pickle](https://docs.python.org/3/library/pickle.html) serializer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "d4ShsBjmIXId",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model_file = open('sa_classifier_lemm.pickle','wb')\n",
    "pickle.dump(model, model_file)\n",
    "model_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LADGM6k8IXId"
   },
   "source": [
    "### Save the model (Colab version)\n",
    "\n",
    "Google Colab doesn't provide direct access to files saved during a notebook session, so we need to save it in [Google Drive](https://drive.google.com) instead. The first time you run this, it will ask for permission to access your Google Drive. Follow the instructions, then wait a few minutes and look for a new folder called \"Colab Output\" in [Drive](https://drive.google.com). Note that Colab does not alway sync to Drive immediately, so check the file update times and re-run this cell if it doesn't look like you have the most revent version of your file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ZPN78S6-IXId"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# if 'google.colab' in sys.modules:\n",
    "#     from google.colab import drive\n",
    "#     drive.mount('/content/gdrive')\n",
    "#     !mkdir -p '/content/gdrive/My Drive/Colab Output'\n",
    "#     model_file = open('/content/gdrive/My Drive/Colab Output/sa_classifier.pickle','wb')\n",
    "#     pickle.dump(model, model_file)\n",
    "#     model_file.flush()\n",
    "#     print('Model saved in /content/gdrive/My Drive/Colab Output')\n",
    "#     !ls '/content/gdrive/My Drive/Colab Output'\n",
    "#     drive.flush_and_unmount()\n",
    "#     print('Re-run this cell if you cannot find it in https://drive.google.com')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "train_sentiment_analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
